{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torchtext in /usr/local/anaconda3/lib/python3.6/site-packages (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/anaconda3/lib/python3.6/site-packages (from torchtext) (0.1.91)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/anaconda3/lib/python3.6/site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/hamzaliaqet/.local/lib/python3.6/site-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/anaconda3/lib/python3.6/site-packages (from torchtext) (4.40.2)\n",
      "Requirement already satisfied, skipping upgrade: torch in /usr/local/anaconda3/lib/python3.6/site-packages (from torchtext) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/hamzaliaqet/.local/lib/python3.6/site-packages (from requests->torchtext) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests->torchtext) (2.6)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hamzaliaqet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "!pip install torchtext --upgrade\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find hashtags.\n",
    "def extract_hashtags(tweet_text):\n",
    "    match_hashtag = re.compile('#\\w+')\n",
    "    hashtags_list = match_hashtag.findall(tweet_text)\n",
    "    hashtags_list_without_hash_symbol = [hashtag[1:] for hashtag in hashtags_list]\n",
    "    return hashtags_list_without_hash_symbol\n",
    "\n",
    "# Split hashtags. Based on Capital letter assumption\n",
    "def get_words_from_hashtags(hashtag):\n",
    "    expanded = [a for a in re.split('([A-Z][a-z]+)', hashtag) if a]\n",
    "    return expanded # If this is providing more words than probablistic approach (wordninja). Use this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-393c3501c27f3263",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create class (instead of a function) so that we don't have to pass \n",
    "# stopwords in every func call\n",
    "\n",
    "class PreprocessTweets(object): \n",
    "    \n",
    "    def __init__(self, _stopwords=[]):\n",
    "        self.stopwords = _stopwords\n",
    "        \n",
    "    def __call__(self, tweet_text): # call this everytime an object of this class is instantiated\n",
    "        ### BEGIN SOLUTION\n",
    "        hashtags = extract_hashtags(tweet_text)\n",
    "\n",
    "        # Remove only 10 chars after t.co/  . Any thing else is meaningful\n",
    "        t_dot_co_url_re = re.compile('https://t.co/\\w{10}')\n",
    "        tweet_text_no_url = t_dot_co_url_re.sub('', tweet_text)\n",
    "\n",
    "        # Remove, 's e.g. teacher's => teacher\n",
    "        re_for_removing_s = re.compile(\"('s)|('S)\") # step 1\n",
    "        tweet_text_no_s = re_for_removing_s.sub('', tweet_text_no_url)\n",
    "\n",
    "        # Remove apostrophe comma. e.g. won't => wont\n",
    "        re_for_removing_apostrophe = re.compile(\"'\") # step 2\n",
    "        tweet_text_no_apostrophe = re_for_removing_apostrophe.sub('', tweet_text_no_s)\n",
    "\n",
    "        tokenized_text = nltk.word_tokenize(tweet_text_no_apostrophe)\n",
    "        \n",
    "        # Keep only unicode chars\n",
    "        re_for_removing_non_alphanumeric_chars = re.compile(\"[a-zA-Z0-9_]+\")\n",
    "        tokens_with_alphanumeric_words = []\n",
    "        for word in tokenized_text:\n",
    "            words_with_alpha_numeric_chars = re_for_removing_non_alphanumeric_chars.findall(word)\n",
    "            tokens_with_alphanumeric_words = tokens_with_alphanumeric_words \\\n",
    "                                             + words_with_alpha_numeric_chars\n",
    "        \n",
    "        # From tokenized text, remove hashtags- otherwise duplicates might occur.    \n",
    "        tokenized_text = [token for token in tokens_with_alphanumeric_words if token not in hashtags]\n",
    "\n",
    "        # Hashtag to words\n",
    "        hashtag_words_extracted = list(map(lambda hashtag: get_words_from_hashtags(hashtag),\n",
    "                                           hashtags))\n",
    "        hashtag_words_in_1D_list = [item for sublist in hashtag_words_extracted \n",
    "                                            for item in sublist]\n",
    "\n",
    "        tokenized_text = tokenized_text + hashtag_words_in_1D_list\n",
    "\n",
    "        # Convert each word to lower case\n",
    "        tokenized_text_lowercase = list(map(lambda word: word.lower(), tokenized_text))\n",
    "\n",
    "        # Lemmatizer\n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemmatized_tokens = list(map(lambda word: str(wnl.lemmatize(word)), tokenized_text_lowercase))\n",
    "\n",
    "\n",
    "        # Stop words removal.\n",
    "        tokens_without_stop_words = [word for word in lemmatized_tokens \\\n",
    "                                    if word not in self.stopwords]\n",
    "        return tokens_without_stop_words\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTweets(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we define our NN (a simple NN with two layers). Emedding layer and linear layer (with 2 nodes- binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PoliticalPartyClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(PoliticalPartyClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL Terminologies \n",
    "Earlier we mentioned that in order to improve performance, NN will tweak its embeddings and other parameters (called weights and biases) by minimizing error gradually. In practice, it takes a small subset of data as input at a time (called `batch`), say 16 examples at a time, and tweaks its parameters once for each batch. \n",
    "\n",
    "If there are 160 examples in our dataset (a small dataset), then NN will see the whole dataset in 10 `iterations` (160/16)- in each iteration it'll see a batch of 10 examples- these 10 iterations will make an `epoch`. \n",
    "\n",
    "We have not created batches of data previously (with SVC). It turns out `PyTorch` takes care of creating random `batches` of data for us by providing us a [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TweetsDataset(Dataset):\n",
    "    \"Given a key, the corresponding example should be returned\"\n",
    "    \n",
    "    def __init__(self, _tweets_csv_file_path, transform=None):\n",
    "        self.dataset_path = _tweets_csv_file_path\n",
    "        self.tweets_df = pd.read_csv(self.dataset_path)\n",
    "        self.transform = transform # for optional preprocessing\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tweets_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        this_tweet = self.tweets_df['tweets'][idx]\n",
    "        party_of_the_author = self.tweets_df['Party'][idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            this_tweet = self.transform(this_tweet)\n",
    "            \n",
    "        return {\n",
    "            'tweet': this_tweet,\n",
    "            'party': party_of_the_author\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTweets(stopwords) # Our Preprocesing which will automatically be applied to each tweet\n",
    "\n",
    "\n",
    "_tweets_dataset = TweetsDataset(_tweets_csv_file_path='UK_MPs_tweets/MPsTweets_from_24Aug_31Aug_2020.csv',\n",
    "                               transform=preprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a pytorch dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(_tweets_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 {'tweet': [('shop', 'sgrstk', 'last', 'keep'), ('across', 'replaced', 'chance', 'fighting'), ('uk', 'cancel', 'today', 'amy'), ('following', 'culture', 'good', 'wishing'), ('government', 'critical', 'luck', 'speedy'), ('guidance', 'thinking', 'getting', 'recovery')], 'party': ['Conservative', 'Conservative', 'Conservative', 'Conservative']}\n"
     ]
    }
   ],
   "source": [
    "# # Display a batch- created by data_loader\n",
    "# # Here you should get tensors (of ints). Not tokens.\n",
    "for i_batch, sample_batched in enumerate(data_loader):\n",
    "    if i_batch == 1:\n",
    "        print(len(sample_batched), sample_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Batch is messed up`:\n",
    "- This batch looks messed up.\n",
    "- There are 4 tweets (which is ok since batch size is 4) but each tweet has exactly 4 tokens put in a tuple.\n",
    "\n",
    "`What it should be`:\n",
    "- A tweet with the number of elements equal to number of words in it (not 4 words).\n",
    "- And instead of words (strings). It should be a Tensor of integers (very similar to numpy array of integers) which NN can handle.\n",
    "\n",
    "- The reason is default `collate_fn` argument in `DataLoader` which doesn't know we're dealing with textual data. One way is we could use classes such as 'Fields', 'examples' defined in [torchtext](https://pytorch.org/text/data.html#fields) library. But those classes are problematic, opaque and confusing to users and will be removed from PyTorch in the next release according to [these release notes of PyTorch](https://github.com/pytorch/text/releases). For these reasons, we decided against using these \"deprecated classes\" to develop this tutorial.\n",
    "- Instead, we'll do very similar to what we did previously with `tfidf` and `word2vec`. Create our own [vocab](https://pytorch.org/text/vocab.html#torchtext.vocab.Vocab) using our newly created `_tweets_dataset`, and then to numericalize (convert words to integers) using [numericalize_tokens_from_iterator](https://pytorch.org/text/data_functional.html#numericalize-tokens-from-iterator)- ultimately getting our tensors of integers which we can give to our embeddingbag layer. And we can put all this in our custom `collate_fn` to generate correct batches. Let's do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom generate batch func- collate_fn\n",
    "\n",
    "class GenerateBatch(object): # Create a class, so that we don't have to pass VOCAB every time.\n",
    "    def __init__(self, VOCAB):\n",
    "        self._VOCAB = VOCAB # Assume vocab exists\n",
    "        \n",
    "    def __call__(self, batch): # dataloader gurantees to provide batch\n",
    "        \n",
    "        # Binary classification 0/1 labels. Converted to tensor. \n",
    "        label = torch.tensor([1 if tweet_object['party'] == 'Conservative' else 0 for tweet_object in batch])\n",
    "        \n",
    "        # Extract tweets texts from this batch. \n",
    "        tweets_text_itr = [tweet_object['tweet'] for tweet_object in batch] # << tweet text is currently 'tokens'\n",
    "        \n",
    "        # Numericalize tokens. Convert tokens to corresponding integers using 'vocab'\n",
    "        tweets_txt_numericalized_gens = numericalize_tokens_from_iterator(self._VOCAB, \n",
    "                                                                    tweets_text_itr)\n",
    "        # Convert tweets to tensors.\n",
    "        tweets_txt_tensors = [torch.from_numpy(np.fromiter(tweet_gen, int)) \n",
    "             for tweet_gen in tweets_txt_numericalized_gens]\n",
    "    \n",
    "        # Embedding layer takes the whole batch (multiple tweets) as inputs. \n",
    "        # Add offsets to pointout where the next tweet begins.\n",
    "        # torch.Tensor.cumsum returns the cumulative sum\n",
    "        # of elements in the dimension dim.\n",
    "        # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "        offsets = [0] + [len(tweet) for tweet in tweets_txt_tensors]\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "      \n",
    "        # Put all tensors in a flat long tensor (Which the embedding layer expects; Read docs.)\n",
    "        text = torch.cat(tweets_txt_tensors) # text is a flat tensor fed to embedding layer next.\n",
    "        return text, offsets, label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counter(dataset_):\n",
    "    '''\n",
    "    Counter object returned is used to build vocabulary. \n",
    "    '''\n",
    "    len_dataset = len(dataset_) \n",
    "    \" Create a Counter object. \"\n",
    "    counter = Counter()\n",
    "    for i in range(len_dataset):\n",
    "        example = dataset_[i]\n",
    "        tweet_txt = example['tweet']\n",
    "        counter.update(tweet_txt)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find counter\n",
    "counter = get_counter(_tweets_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now define _VOCAB\n",
    "from torchtext.vocab import Vocab\n",
    "_VOCAB = Vocab(counter) # counter not yet defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch = GenerateBatch(_VOCAB)\n",
    "# generate_batch # << collate_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a dataloader with custom generate batch func\n",
    "data_loader = DataLoader(_tweets_dataset, batch_size=4, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (tensor([ 402,   30,    6,  332,    3,  806,  164,   61,  184,  253,  196,    5,\n",
      "          17,  267,  298,  402,   17, 6543, 6486, 1822,  371, 3578, 1314, 1956,\n",
      "        6016,  241,  336, 2758, 2474,  575,   49,  783,    8,   67,  749,  355,\n",
      "        2710, 1121,  587,   20,  164, 1381, 5724, 1127, 1931,  199]), tensor([ 0, 17, 30, 40]), tensor([1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# # Display a batch- created by data_loader\n",
    "# # Here you should get tensors (of ints). Not tokens.\n",
    "for i_batch, sample_batched in enumerate(data_loader):\n",
    "    if i_batch == 1:\n",
    "        print(len(sample_batched), sample_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. All tweets converted to tensor of integers. And concatenated.\n",
    "Labels converted to integers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = get_counter(_tweets_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab from counter \n",
    "_VOCAB = Vocab(counter) # Make vocabulary using counter\n",
    "# print(_vocab.stoi) # prints unique tokens and index assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `collate_fn `\n",
    "\n",
    "How each batch should be created can be customized in `collate_fn`. Here, we'll generate a batch (tweets, offsets, labels) where each of them is a `Tensor` of containing integers. Note that tweets shouldn't contain tokens but corresponding integers assigned based on the mapping (`Vocab`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class GenerateBatch(object): # Create a class, so that we don't have to pass VOCAB every time.\n",
    "    def __init__(self, VOCAB):\n",
    "        self._VOCAB = VOCAB\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        # Binary classification 0/1 labels. Converted to tensor. \n",
    "        label = torch.tensor([1 if tweet_object['party'] == 'Conservative' else 0 for tweet_object in batch])\n",
    "        \n",
    "        # Extract tweets texts from this batch. \n",
    "        tweets_text_itr = [tweet_object['tweet'] for tweet_object in batch] # << tweet text is currently 'tokens'\n",
    "        \n",
    "        # Numericalize tokens. Convert tokens to corresponding integers using 'vocab'\n",
    "        tweets_txt_numericalized_gens = numericalize_tokens_from_iterator(self._VOCAB, \n",
    "                                                                    tweets_text_itr)\n",
    "        # Convert tweets to tensors.\n",
    "        tweets_txt_tensors = [torch.from_numpy(np.fromiter(tweet_gen, int)) \n",
    "             for tweet_gen in tweets_txt_numericalized_gens]\n",
    "    \n",
    "        # Embedding layer takes the whole batch (multiple tweets) as inputs. \n",
    "        # Add offsets to pointout where the next tweet begins.\n",
    "        # torch.Tensor.cumsum returns the cumulative sum\n",
    "        # of elements in the dimension dim.\n",
    "        # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "        offsets = [0] + [len(tweet) for tweet in tweets_txt_tensors]\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "      \n",
    "        # Put all tensors in a flat long tensor (Which the embedding layer expects; Read docs.)\n",
    "        text = torch.cat(tweets_txt_tensors) # text is a flat tensor fed to embedding layer next.\n",
    "        return text, offsets, label\n",
    "    \n",
    "generate_batch = GenerateBatch(_VOCAB)\n",
    "# generate_batch # << collate_fn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, instead of offsets, optionally, we could have padded and passed fixed length 2D tensors like this (Bags, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display a batch.\n",
    "# # Here you should get tensors (of ints). Not tokens.\n",
    "# for i_batch, sample_batched in enumerate(dataloader):\n",
    "#     if i_batch == 1:\n",
    "#         break\n",
    "#     print(len(sample_batched), sample_batched)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This remaining code is pretty generic; You can also find it [here](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(_VOCAB)\n",
    "EMBED_DIM = 32\n",
    "N_CLASS = 2 # Binary Classification \n",
    "BATCH_SIZE = 16\n",
    "model = PoliticalPartyClassificationModel(VOCAB_SIZE, EMBED_DIM, N_CLASS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,  collate_fn=generate_batch)\n",
    " \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0367(train)\t|\tAcc: 79.0%(train)\n",
      "\tLoss: 0.0074(valid)\t|\tAcc: 85.5%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0317(train)\t|\tAcc: 82.2%(train)\n",
      "\tLoss: 0.0037(valid)\t|\tAcc: 84.4%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0235(train)\t|\tAcc: 85.3%(train)\n",
      "\tLoss: 0.0036(valid)\t|\tAcc: 88.2%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0168(train)\t|\tAcc: 89.8%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tAcc: 88.7%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0119(train)\t|\tAcc: 93.0%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tAcc: 90.2%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0077(train)\t|\tAcc: 96.1%(train)\n",
      "\tLoss: 0.0039(valid)\t|\tAcc: 89.6%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0047(train)\t|\tAcc: 97.9%(train)\n",
      "\tLoss: 0.0037(valid)\t|\tAcc: 89.3%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0030(train)\t|\tAcc: 98.8%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 90.5%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0021(train)\t|\tAcc: 99.4%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 90.8%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0016(train)\t|\tAcc: 99.6%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tAcc: 90.8%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 10\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(_tweets_dataset) * 0.8)\n",
    "val_len = int(len(_tweets_dataset) * 0.1)\n",
    "\n",
    "sub_train_, sub_valid_, sub_test = \\\n",
    "    random_split(_tweets_dataset, [train_len, \n",
    "                                   val_len, \n",
    "                                   len(_tweets_dataset) - (val_len + train_len)])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0036(test)\t|\tAcc: 87.3%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(sub_test)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict an example\n",
    "Take recent tweets from [here](https://www.mpsontwitter.co.uk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author (MP) of this tweet belongs to 'Labour' party.\n"
     ]
    }
   ],
   "source": [
    "political_party_label = {\n",
    "    0 : \"Labour\",\n",
    "    1 : \"Conservative\"\n",
    "}\n",
    "\n",
    "def predict(text, model, vocab):\n",
    "    preprocessed_text = preprocess(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in preprocessed_text])\n",
    "        offset = torch.tensor([0])\n",
    "        output = model(text, offset)\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "tweet_txt = \"Julian Assange's extradition hearing resumes today. He could be sent to the USA for his journalism including the exposing of US war crimes. Along with Amnesty & the UN Special Rapporteur on Torture, I oppose this extradition. All supporters of a free press should oppose it too. https://pic.twitter.com/0MwHW7l0pn\"\n",
    "print(\"The author (MP) of this tweet belongs to '%s' party.\" %political_party_label[predict(tweet_txt, model, \n",
    "                                                 _VOCAB)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author's Notes\n",
    "##### Other Notes\n",
    "- Embedding layer once instanitated, the instantiated object can accept args (Bags, offsets) i.e. in \\__call__ method. Offsets determine starting point of next bag (seq). This should be in collate_fn. \n",
    "- Your dataset currently contains tokens- you need a lookup table from [torchtext](https://pytorch.org/text/index.html) to get integers and then you make a tensor of it. In create dataset, convert it to integers.\n",
    "- The key is to read documentation of embedding layer. \n",
    "- Fields is depracated\n",
    "\n",
    "##### Notes on Numericalizing\n",
    "1. [Build vocab from iterator](https://pytorch.org/text/vocab.html#torchtext.vocab.build_vocab_from_iterator) OR [build vocab from a counter](https://pytorch.org/text/vocab.html#torchtext.vocab.Vocab)\n",
    "\n",
    "2. [Numericalize i.e. yield a list of ids from a token iterator with the vocab from step 1](https://pytorch.org/text/data_functional.html#torchtext.data.functional.numericalize_tokens_from_iterator). These numerical values should be returned by `collat_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
