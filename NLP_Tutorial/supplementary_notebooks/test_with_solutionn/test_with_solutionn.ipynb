{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Natural Language Processing\n",
    "Here, we'll predict political party ('Conservative' or 'Labour') of a Member of Parliament (MP) of UK Parliament based on his/her tweets. The data (tweets of ~500 MPs since Aug 23- 2020) were extracted using V2 of Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f7d286c15b1e9ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hamzaliaqet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Library imports.\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nose.tools as test_\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-333c2f053e3e69a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets: 3464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>MP_twitter_username</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @MoJGovUK: New courtroom protections in som...</td>\n",
       "      <td>edwardtimpson</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanks for the warm welcome and for all that y...</td>\n",
       "      <td>JBrokenshire</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Shaunaryallx @Taurus8Gemini That really isn’t...</td>\n",
       "      <td>RhonddaBryant</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @KateGreenSU: Today on @GMB I emphasised ho...</td>\n",
       "      <td>Bill_Esterson</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SebastianEPayne: Corbyn aide Andrew Murray...</td>\n",
       "      <td>patmcfaddenmp</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets MP_twitter_username  \\\n",
       "0  RT @MoJGovUK: New courtroom protections in som...       edwardtimpson   \n",
       "1  Thanks for the warm welcome and for all that y...        JBrokenshire   \n",
       "2  @Shaunaryallx @Taurus8Gemini That really isn’t...       RhonddaBryant   \n",
       "3  RT @KateGreenSU: Today on @GMB I emphasised ho...       Bill_Esterson   \n",
       "4  RT @SebastianEPayne: Corbyn aide Andrew Murray...       patmcfaddenmp   \n",
       "\n",
       "          Party  \n",
       "0  Conservative  \n",
       "1  Conservative  \n",
       "2        Labour  \n",
       "3        Labour  \n",
       "4        Labour  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file dataset into a dataframe\n",
    "tweets_df = pd.read_csv(\"UK_MPs_tweets/MPsTweets_from_24Aug_31Aug_2020.csv\")\n",
    "print('total tweets:',len(tweets_df)) # total tweets\n",
    "tweets_df = tweets_df.sample(frac=1, random_state=1).reset_index(drop=True) # shuffle rows\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sklearn TFIDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) can take raw tweets, proprocess them using a generic way, and return TFIDF features which we can easily feed to our classifier. Let's do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Full ML (NLP Classification) Pipeline Below:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score out of 1 = 0.9925742574257426\n",
      "test score out of 1 = 0.8798076923076923\n"
     ]
    }
   ],
   "source": [
    "# Extract tweets text (raw features) and labels.\n",
    "_raw_features_tweets = tweets_df['tweets']\n",
    "_labels = tweets_df['Party']\n",
    "\n",
    "# Instantiate tfidf vectorizer\n",
    "_sklearn_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and Transform (i.e. get TFIDF features)\n",
    "_sklearn_tfidf_features = _sklearn_vectorizer.fit_transform(_raw_features_tweets)\n",
    "\n",
    "# Split into train/test sets. Train used for training. Test (unseen examples) for testing.\n",
    "_X_train, _X_test, _y_train, _y_test = train_test_split(_sklearn_tfidf_features, _labels,\n",
    "                                        test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Instantiate a classifier (SVC).\n",
    "_clf = SVC()\n",
    "\n",
    "# Train the classifier on train data.\n",
    "_clf.fit(_X_train, _y_train)\n",
    "# Performance (R2 score) on train data.\n",
    "print('train score out of 1 =', _clf.score(_X_train, _y_train) )\n",
    "# Performance (R2 score) on test data.\n",
    "print('test score out of 1 =', _clf.score(_X_test, _y_test)) # Highest possible = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple. TFIDF vectorizer did most of the work for us. However, it turns out for most practical problems in NLP, you'd want to do your custom preprocessing i.e. how exactly do you want to convert raw text into clean tokens and then use those clean tokens to convert to TFIDF matrix. In this problem, accuracy will slightly improve on test data. \n",
    "\n",
    "`Spoiler:` `This processing function we write next will also be used again when we solve this same problem using Deep Learning (PyTorch))` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9fde8d0ae0d440b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# You may define any helper functions in this cell or any other cell if needed.\n",
    "### BEGIN SOLUTION\n",
    "# First find hashtags.\n",
    "def extract_hashtags(tweet_text):\n",
    "    match_hashtag = re.compile('#\\w+')\n",
    "    hashtags_list = match_hashtag.findall(tweet_text)\n",
    "    hashtags_list_without_hash_symbol = [hashtag[1:] for hashtag in hashtags_list]\n",
    "    return hashtags_list_without_hash_symbol\n",
    "\n",
    "# Split hashtags. Based on Capital letter assumption\n",
    "def get_words_from_hashtags(hashtag):\n",
    "    # Extract word if you see 'one' capital letter and any number \n",
    "    # of small letters next to it.\n",
    "    expanded = [a for a in re.split('([A-Z][a-z]+)', hashtag) if a] \n",
    "    return expanded # If this is providing more words than probablistic approach (wordninja). Use this.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a51365832d0514fe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "In the function below, given a text, return preprocessed text such that:\n",
    "- it is tokenized.\n",
    "- it only contains unicode chars (alphabets, digits or underscore only).\n",
    "- Remove trailing 's e.g. `teacher's` becomes `teacher`\n",
    "- apostrophe is removed. e.g. `don't` becomes `dont`\n",
    "- tokens are lemmatized\n",
    "- Contains no stopwords if list of stopwords is provided.\n",
    "- Extract words from hashtag. `#BackToSchool` becomes `back, to, school` (if stopwords list is empty otherwise `to` will also be removed)\n",
    "- each token is lowercased.\n",
    "- All valid `t.co` URLs are removed. \n",
    "\n",
    "`Hints:`\n",
    "- For hashtags, each word starts with a capital letter; Assume that it's always true.\n",
    "- For URLs, pay attention to length of a valid `t.co` URL.\n",
    "- If the above preprocessing points are applied in the right order, solving this problem is simpler.\n",
    "\n",
    "\n",
    "`More Hints/Pointers with examples are in the test cases below; Comments are also written for clarifications.`\n",
    "\n",
    "`A Hint from Twitter:` \n",
    "'You cannot add spaces or punctuation in a hashtag, or it will not work properly. (Twitter)'\n",
    "\n",
    "\n",
    "`Useful Libraries:`\n",
    "1. [NLTK](https://www.nltk.org)\n",
    "2. [Regular Expressions](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Right Order:` (Other ways are also possible. Let's us know if figured even simpler way)\n",
    "1. separate out hashtags e.g. [#BackToSchool] from tweet\n",
    "2. All valid `t.co` urls contain 10 chars in the suffix. Use regex with this pattern.\n",
    "3. Remove 's using regex. \n",
    "4. Remove apostrophe using regex. \n",
    "5. Tokenize using nltk\n",
    "6. Keep only alphanumeric chars and underscore again simply using a regex.\n",
    "7. Remove full hashtags (e.g. BackToSchool) from tokens and add splitted words (e.g. [back, to, school]) to avoid duplicates.\n",
    "8. Finally convert each word to lower case, lemmatize and remove it if a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-393c3501c27f3263",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create class (instead of a function) so that we don't have to pass \n",
    "# stopwords in every func call\n",
    "\n",
    "class PreprocessTweets(object): \n",
    "    \n",
    "    def __init__(self, _stopwords=[]):\n",
    "        self.stopwords = _stopwords\n",
    "        \n",
    "    def __call__(self, tweet_text): # call this everytime an object of this class is instantiated\n",
    "        ### BEGIN SOLUTION\n",
    "        hashtags = extract_hashtags(tweet_text)\n",
    "\n",
    "        # Remove only 10 chars after t.co/  . Any thing else is meaningful\n",
    "        t_dot_co_url_re = re.compile('https://t.co/\\w{10}')\n",
    "        tweet_text_no_url = t_dot_co_url_re.sub('', tweet_text)\n",
    "\n",
    "        # Remove, 's e.g. teacher's => teacher\n",
    "        re_for_removing_s = re.compile(\"('s)|('S)\") # step 1\n",
    "        tweet_text_no_s = re_for_removing_s.sub('', tweet_text_no_url)\n",
    "\n",
    "        # Remove apostrophe comma. e.g. won't => wont\n",
    "        re_for_removing_apostrophe = re.compile(\"'\") # step 2\n",
    "        tweet_text_no_apostrophe = re_for_removing_apostrophe.sub('', tweet_text_no_s)\n",
    "\n",
    "        tokenized_text = nltk.word_tokenize(tweet_text_no_apostrophe)\n",
    "        \n",
    "        # Keep only unicode chars\n",
    "        re_for_removing_non_alphanumeric_chars = re.compile(\"[a-zA-Z0-9_]+\")\n",
    "        tokens_with_alphanumeric_words = []\n",
    "        for word in tokenized_text:\n",
    "            words_with_alpha_numeric_chars = re_for_removing_non_alphanumeric_chars.findall(word)\n",
    "            tokens_with_alphanumeric_words = tokens_with_alphanumeric_words \\\n",
    "                                             + words_with_alpha_numeric_chars\n",
    "        \n",
    "        # From tokenized text, remove hashtags- otherwise duplicates might occur.    \n",
    "        tokenized_text = [token for token in tokens_with_alphanumeric_words if token not in hashtags]\n",
    "\n",
    "        # Hashtag to words\n",
    "        hashtag_words_extracted = list(map(lambda hashtag: get_words_from_hashtags(hashtag),\n",
    "                                           hashtags))\n",
    "        hashtag_words_in_1D_list = [item for sublist in hashtag_words_extracted \n",
    "                                            for item in sublist]\n",
    "\n",
    "        tokenized_text = tokenized_text + hashtag_words_in_1D_list\n",
    "\n",
    "        # Convert each word to lower case\n",
    "        tokenized_text_lowercase = list(map(lambda word: word.lower(), tokenized_text))\n",
    "\n",
    "        # Lemmatizer\n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemmatized_tokens = list(map(lambda word: str(wnl.lemmatize(word)), tokenized_text_lowercase))\n",
    "\n",
    "\n",
    "        # Stop words removal.\n",
    "        tokens_without_stop_words = [word for word in lemmatized_tokens \\\n",
    "                                    if word not in self.stopwords]\n",
    "        return tokens_without_stop_words\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e458e596b7854e3b",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"  >>>>> Testing Without Stopwords <<<<< \"\n",
    "#___________________________________________________\n",
    "preprocess = PreprocessTweets()\n",
    "test_.eq_ ((preprocess(\"teacher's\",)), ['teacher'])\n",
    "test_.eq_ ((preprocess(\"Let's get them back to classroom\",)), ['let', 'get', 'them', 'back', 'to', 'classroom'])\n",
    "\n",
    "# Underscore is a unicode char. It should be in the output\n",
    "test_.eq_ ((preprocess(\"@gone_too_far__ Read the article \",)), \n",
    "           ['gone_too_far__', 'read', 'the', 'article'])\n",
    "\n",
    "test_.eq_ ((preprocess(\"Good luck👍👍,Alex_Stafford\")), ['good', 'luck', 'alex_stafford'])\n",
    "test_.eq_  (preprocess(\"$%#hello-^^world!!\"), ['world', 'hello'])\n",
    "#___________________________________________________\n",
    "\n",
    "# URL \n",
    "test_.eq_ (preprocess(\"Register 👉 https://t.co/dCjWFDKoKO\"), ['register'])\n",
    "# There is text embedded after the URL. Extract it.\n",
    "test_.eq_ (preprocess(\"https://t.co/MAdn2K1PwH,Alex_Stafford,Conservative\"), \\\n",
    "           ['alex_stafford', 'conservative'])\n",
    "test_.eq_ (preprocess(\"Register 👉 https://t.co/3zi5fXSrOkHello-Griffitha,Conservative\"), \\\n",
    "           ['register', 'hello', 'griffitha', 'conservative'])\n",
    "\n",
    "# URL contains non-unicode chars: Invalid URL. Don't Remove it entirely.\n",
    "test_.eq_ (preprocess(\"https://t.co/ZhEyÄ¶aaaa\"), ['http', 't', 'co', 'zhey', 'aaaa'])\n",
    "\n",
    "test_.eq_ ((preprocess(\"I'm live now with @AngelaRayner of @UKLabour as I\")), \n",
    "           ['im', 'live', 'now', 'with', 'angelarayner', 'of', 'uklabour', 'a', 'i'])\n",
    "\n",
    "test_.eq_ (preprocess(\"Don't…',GwynneMP,Labour\"), ['dont', 'gwynnemp', 'labour'])\n",
    "\n",
    "#___________________________________________________\n",
    "\n",
    "# Hashtags Split at words. For simplicity, \n",
    "# assume that next word starts with a capital letter \n",
    "\n",
    "test_.eq_ (preprocess(' #ShopLocal'), ['shop', 'local'])\n",
    "test_.eq_ (preprocess(' #EatOutHelpOut'), ['eat', 'out', 'help', 'out'])\n",
    "test_.eq_ (preprocess('#InternationalDayoftheDisappeared'),\n",
    "           ['international', 'dayofthe', 'disappeared'])\n",
    "'''\n",
    "\n",
    "'Aside':  # It's possible to split the above example into:\n",
    "['International', 'Day', 'of', 'the', 'Disappeared'] using probablistic models. \n",
    "We' won't ask you to do so for the sake of this test- primarily for simplicity.\n",
    "If interested, check this small module out >>> pip install wordninja\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\"  >>>>> Testing With Stopwords <<<<< \"\n",
    "\n",
    "#___________________________________________________\n",
    "\n",
    "preprocess_with_stopwords = PreprocessTweets(stopwords)\n",
    "test_.eq_ ((preprocess_with_stopwords(\"LET'S get them back to classroom\")), \n",
    "           ['let', 'get', 'back', 'classroom'])\n",
    "\n",
    "test_.eq_ ((preprocess_with_stopwords(\"I'm live now with @AngelaRayner of @UKLabour as I\")), \n",
    "                                ['im', 'live', 'angelarayner', 'uklabour'])\n",
    "# https etc are stopwords. Now they'll be removed.\n",
    "test_.eq_ (preprocess_with_stopwords(\"https://t.co/ZhEyÄ¶cccc\"), ['zhey', 'cccc'])\n",
    "\n",
    "test_.eq_ ((preprocess_with_stopwords('https://t.co/vVzR52faue\",Afzal4Gorton,Labour')), \n",
    "                    ['afzal4gorton', 'labour'])\n",
    "#___________________________________________________\n",
    "\n",
    "test_.eq_ (\n",
    "    preprocess_with_stopwords\n",
    "    ( # Don't confuse: backslashes are to break the line (not part of the tweet text)\n",
    "        \"✈ It's back to London today for a new Parliamentary\\\n",
    "        Session🗓 This Week I'm👨‍⚕️ In Health Questions🐟  \\\n",
    "        In the Fisheries Bill Debate🙋‍♂️ Question Number     \\\n",
    "        One in PMQs󠁢󠁳󠁣󠁴󠁿 Talking more Fish in Scottish Affairs       \\\n",
    "        Committee 🚢 Raising issue of Freedom of Navigation on   \\\n",
    "        the South China Sea https://t.co/aisLmptsCR\"\n",
    "    ), \n",
    "    [\n",
    "        'back',\n",
    "        'london',\n",
    "        'today',\n",
    "        'new',\n",
    "        'parliamentary',\n",
    "        'session',\n",
    "        'week',\n",
    "        'im',\n",
    "        'health',\n",
    "        'question',\n",
    "        'fishery',\n",
    "        'bill',\n",
    "        'debate',\n",
    "        'question',\n",
    "        'number',\n",
    "        'one',\n",
    "        'pmqs',\n",
    "        'talking',\n",
    "        'fish',\n",
    "        'scottish',\n",
    "        'affair',\n",
    "        'committee',\n",
    "        'raising',\n",
    "        'issue',\n",
    "        'freedom',\n",
    "        'navigation',\n",
    "        'south',\n",
    "        'china',\n",
    "        'sea'\n",
    "    ]\n",
    ")\n",
    "#___________________________________________________\n",
    "### BEGIN HIDDEN TESTS\n",
    "test_.eq_ (\n",
    "    preprocess(\n",
    "        'On #InternationalDayoftheDisappeared,        \\\n",
    "        I think of the loved ones and friends of       \\\n",
    "        the hundreds of thousands of people who have    \\\n",
    "        “disappeared” in #Syria - imprisoned or murdered \\\n",
    "        by the Dictator Assad. Thank you Caesar for the   \\\n",
    "        photos you smuggled out to ensure the world knew   \\\n",
    "        the truth https://t.co/Lg5jm8Iwu9.', \n",
    "    ),\n",
    "    [\n",
    "        'on',\n",
    "        'i',\n",
    "        'think',\n",
    "        'of',\n",
    "        'the',\n",
    "        'loved',\n",
    "        'one',\n",
    "        'and',\n",
    "        'friend',\n",
    "        'of',\n",
    "        'the',\n",
    "        'hundred',\n",
    "        'of',\n",
    "        'thousand',\n",
    "        'of',\n",
    "        'people',\n",
    "        'who',\n",
    "        'have',\n",
    "        'disappeared',\n",
    "        'in',\n",
    "        'imprisoned',\n",
    "        'or',\n",
    "        'murdered',\n",
    "        'by',\n",
    "        'the',\n",
    "        'dictator',\n",
    "        'assad',\n",
    "        'thank',\n",
    "        'you',\n",
    "        'caesar',\n",
    "        'for',\n",
    "        'the',\n",
    "        'photo',\n",
    "        'you',\n",
    "        'smuggled',\n",
    "        'out',\n",
    "        'to',\n",
    "        'ensure',\n",
    "        'the',\n",
    "        'world',\n",
    "        'knew',\n",
    "        'the',\n",
    "        'truth',\n",
    "        'international',\n",
    "        'dayofthe',\n",
    "        'disappeared',\n",
    "        'syria'\n",
    "    ])\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7645ce04b7456494",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract tweets text (raw features) and labels.\n",
    "raw_features_tweets = tweets_df['tweets']\n",
    "labels = tweets_df['Party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b57602a35a0baa3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess features using preprocess.\n",
    "preprocessed_features = raw_features_tweets.apply(func=lambda tweet_text: preprocess(tweet_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2baddb12a757281",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rt, mojgovuk, new, courtroom, protection, in,...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thanks, for, the, warm, welcome, and, for, al...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[shaunaryallx, taurus8gemini, that, really, is...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rt, kategreensu, today, on, gmb, i, emphasise...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[rt, sebastianepayne, corbyn, aide, andrew, mu...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets         Party\n",
       "0  [rt, mojgovuk, new, courtroom, protection, in,...  Conservative\n",
       "1  [thanks, for, the, warm, welcome, and, for, al...  Conservative\n",
       "2  [shaunaryallx, taurus8gemini, that, really, is...        Labour\n",
       "3  [rt, kategreensu, today, on, gmb, i, emphasise...        Labour\n",
       "4  [rt, sebastianepayne, corbyn, aide, andrew, mu...        Labour"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put preprocessed features and labels together again.\n",
    "preprocessed_df = pd.concat([preprocessed_features, labels], axis=1)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36224999e4a333c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split into train/test dataset.\n",
    "train_df, test_df = train_test_split(preprocessed_df, test_size=0.3, \n",
    "                                   random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a651fbf4b3f26e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>[rt, emilythornberry, and, for, anyone, that, ...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>[rt, mennewsdesk, breaking, stockport, is, bei...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>[davdotfo, stopinstockport, wednesday, i, m, t...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>[thought, i, d, pop, along, and, see, how, com...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>[rt, sgmacleanauthor, anyone, who, remembers, ...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets         Party\n",
       "1042  [rt, emilythornberry, and, for, anyone, that, ...        Labour\n",
       "1601  [rt, mennewsdesk, breaking, stockport, is, bei...        Labour\n",
       "3122  [davdotfo, stopinstockport, wednesday, i, m, t...        Labour\n",
       "2831  [thought, i, d, pop, along, and, see, how, com...  Conservative\n",
       "2927  [rt, sgmacleanauthor, anyone, who, remembers, ...        Labour"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how train set looks like.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features (TFIDF Vectorization)\n",
    "### Question \n",
    "Create TFIDF matrix using [TfidfVectorizer of sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "1. [`Hint`](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/)\n",
    "2. [`Documentation elaborating the hint`](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db0c49f6ae3dcc64",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_TFIDF_vectorizer(train_data):\n",
    "    '''\n",
    "    args: train_data -> pandas.core.series.Series\n",
    "    return: Fitted (not transformed) TFIDF Vectorizer -> i.e.\n",
    "                        (sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    def do_nothing(doc): return doc\n",
    "\n",
    "    # Instantiate a TFIDF vectorizer\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=do_nothing,\n",
    "        preprocessor=do_nothing,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    tfidf.fit(train_data)\n",
    "    return tfidf\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-249a0891d7a4bf54",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract Train tweets text\n",
    "train_corpus = train_df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-576ca6e291865a31",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf__ = fit_TFIDF_vectorizer(train_corpus)\n",
    "X_train__ = tfidf__.transform(train_corpus)\n",
    "test_.ok_ ((X_train__.nonzero()[:5][0][-15:] == np.asarray(\n",
    "    [\n",
    "        2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, \n",
    "        2423, 2423, 2423,2423, 2423, 2423, 2423\n",
    "    ]\n",
    ")).all())\n",
    "### BEGIN HIDDEN TESTS\n",
    "test_.ok_ ((X_train__.nonzero()[-100:][0][-100:] == np.asarray(\n",
    "    [\n",
    "        2419, 2419, 2420, 2420, 2420, 2420, 2420, 2420, 2420, 2420, 2420,\n",
    "        2420, 2420, 2420, 2420, 2420, 2420, 2420, 2420, 2420, 2420, 2421,\n",
    "        2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421,\n",
    "        2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421,\n",
    "        2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421,\n",
    "        2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421, 2421,\n",
    "        2421, 2422, 2422, 2422, 2422, 2423, 2423, 2423, 2423, 2423, 2423,\n",
    "        2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423,\n",
    "        2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423, 2423,\n",
    "        2423\n",
    "    ]\n",
    ")).all())\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8afa649b50c9333",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf = fit_TFIDF_vectorizer(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a65f44f076af521",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the transform method, actual TFIDF matrix will be created.\n",
    "X_train = tfidf.transform(train_corpus)\n",
    "# Convert labels to integers [0,1].\n",
    "Train_labels = train_df['Party']\n",
    "y_train = [1 if l == 'Conservative' else 0 for l in Train_labels]\n",
    "y_train[:5] # First 5 labels. 1 for conservative. 0 for labour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e6e03dbbe6f5afa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a SVC.\n",
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87101cb9427bd1cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on train data.\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6512d42a7d4bc0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884488448844885"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance (R2 score) on train data.\n",
    "clf.score(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e30235930b1a67a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Make TFIDF features of test data.\n",
    "test_corpus = test_df['tweets']\n",
    "X_test = tfidf.transform(test_corpus)\n",
    "Test_labels = test_df['Party']\n",
    "# Integer labels.\n",
    "y_test = [1 if l == 'Conservative' else 0 for l in Test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ad068044071010f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8836538461538461"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance (R2 score) on test data.\n",
    "clf.score(X_test, y_test) # Highest possible = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b4ee4e31b58791b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test) # already defined- notebook previously was run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffe162c26621a7f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8836538461538461"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on test data.\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement in accuracy in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Remarks:`\n",
    "The classifier, for example, may learn that if a tweet mentions something negative about Boris Jhonson ('Conservative' party ), then it's more likely the author of that tweet is `not` of 'Conservative' party. It learned all that from the training examples with labels we provided. And the classifier's prediction is correct about $~82\\%$ of the times."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
